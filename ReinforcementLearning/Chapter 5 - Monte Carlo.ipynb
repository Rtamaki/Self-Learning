{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 5: Monte Carlo Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5.1 - Why does the estimated value function jump from the last two rows in the rear?\n",
    "\n",
    "\n",
    "The value function jump at the last two rows in the rear because on those rows the player has either 20 or 21 points summed in the hand. If the user has 21 point then he will most likely win or (at worst and quite unlikely) get a draw.  With the current policy of sticking until the player has 20 or 21 points in the hand, if the user will stick and not buy any more cards. As the likelyhood of the dealer obtaining 21 is low, this state is highly valued in this policy\n",
    "\n",
    "However, in any other state, the player will buy cards until she has 20 or 21 points. This policy is unlikely to win for any other state other than 20 and 21, resulting in the value diagram of figure 5.1\n",
    "\n",
    "On the most left side column, it is when the dealer showed an ace (value shown is 1), which decreases the chance that the player will come to the 'stick strategy' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5.2 - Would every-visit MC output a different value functions than the first-visit MC?\n",
    "\n",
    "The difference between every-visit and first-visit MC is whether they include the return of the first visit of a state in the estimates of the value function. As the policy is fixed the only the difference between the value function would come from the first visit, which can be neglected after 500,000 episodes. Indeed as we are averaging the values of the estates\n",
    "Vs == AVG(V0, V1, ..., Vn) = E[Vx]\n",
    "\n",
    "But, E[V0] = E[V1] = E[V2] = ... = E[Vn], so that Vs' = AVG[V1, V2, ..., Vn] and E[Vs' - Vs] = E[E[V0] - V0]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5.3 - What is the backup diagram for the Monte Carlo estimation of Qpi?\n",
    "\n",
    "The Backup Diagram for Monte Carlo is similar to the Backup Diagram used on the Dynamic Programming, with the exception that instead of having the initial state we are interested, we have the state-action pair (s,a) connected to future states (s') through the arrow r.\n",
    "\n",
    "\n",
    "**Question:** On MC MDP, do we assume greedy policy for the evaluation of the next state value similarly to what was done on the Dynamic Programming?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5.4 - How would we compute the more efficiently the average return of the states in a MC MDP?\n",
    "\n",
    "In the same way that was done on the Multi-armed Bandits problem. One can calculate the average by simply holding just 2 values and computing the following calculation:\n",
    "\n",
    "- Current average: CurrAvg\n",
    "- Number of samples of current average: N\n",
    "- New sample: v\n",
    "\n",
    "```\n",
    "def NewAverage(CurrAvg, N, V):\n",
    "    if N == 0:\n",
    "        return V, 1\n",
    "    else:\n",
    "        NewAvg = (CurrAvg*N + V)/(N+1)\n",
    "        return NewAvg, N+1 \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5.5 - What are the values of the first-visit and every-visit estimators of the value of the non-terminal state?\n",
    "\n",
    "The difference between every-visit and first-visit estimators is that the first one doesn't include the first state visit in the estimation of the value of the state.\n",
    "\n",
    "- `t = (T-1) = 9`: G <- 0*1 + 1 = 1, Thus Vs = [1]\n",
    "- `t = (T-2) = 8`: G <- 1*1 + 1 = 2, Thus Vs = [1, 2]\n",
    "- `t = (T-3) = 7`: G <- 2*1 + 1 = 3, Thus Vs = [1, 2, 3]\n",
    "- (...)\n",
    "- `t = (T-8) = 2`: G <- 9*1 + 1 = 9, Thus Vs = [1, 2, 3, 4, 5, 6, 7, 8, 9] -> avg(Vs) = 5\n",
    "- `t = (T-9) = 1`: G <- 9*1 + 1 = 10, Thus Vs = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] -> avg(Vs) = 5.5\n",
    "\n",
    "Which means that the estimate for the first-visit estimator is 5 while for the every-visit estimator is 5.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5.6 - What is the equivalent *weighted importance sampling* but for q(s,a) instead of v(s)?\n",
    "\n",
    "For q(s,a) the analagous *weighted importance sampling* is almost the same, with the exception that we don't include the very first term of P(At=a, St=s) since that is necessarily 1 in the case (we are evaluating the value of the action and assuming that all further actions are taken assuming the policy). As a consequence\n",
    "\n",
    "$ Q(St=s, At=a) = \\frac{\\sum_{t\\epsilon T(t+1)}{p_{t:T-1}}{G_t}}{\\sum_{t\\epsilon T(t+1)}{p_{t:T-1}}} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5.7 - Why did the error of the weighted importance sampling first increased and then decreased with the number of episodes?\n",
    "\n",
    "**I don't know**, but maybe: the error considered in the plot is the Mean Squared Error of the estimated value of the initial state for the target-policy to the true value.\n",
    "As target- and behaviour-policies have distinct P(a|s) and with the initial low-number of samples for the states-values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5.8 - Would the variance of the MC Method used still be infinite if instead of first-visit MC an every-visit MC was used?\n",
    "\n",
    "Yes, because the reason why the variance can be infinite isn't due to the values being sampled (which usually have bounded variance), but because the *importance sample* ratio is unbounded for ordinary importance sampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5.9 - Modifying the algorithm for first-visit MC policy evalution (Section 5.1) to use incremental implementation for sample averages\n",
    "\n",
    "The pseudo-code for an incremental implementation of the first-visit MC prediction algorithm from Section 5.1 is:\n",
    "\n",
    "Input: a policy $\\pi$ to be evaluated\n",
    "Initialize\n",
    "\n",
    " - $ V(s)\\in R, for\\ all\\ s\\in S$\n",
    " - $Return(s) \\leftarrow$ list of zeros for all $s\\in S$\n",
    " - $N(s) \\leftarrow$ list of 0 for all $s\\in S$, which holds the number of times the state was visited\n",
    " \n",
    "Loop forever (for each episode):\n",
    "\n",
    "- Generate an episode following $\\pi$: $S_0, A_0, R_1, S_1, A_1, R_2, ..., S_{T-1}, A_{T-1}, R_T$\n",
    "- G $\\leftarrow$ 0\n",
    "- Loop for each step of the episode, $ t=T-1, T-2, ..., 0$\n",
    "    - G $\\leftarrow \\gamma G + R_{t+1}$\n",
    "    - Unless $S_t$ appears in $S_0, S_1, ..., S_{T-1}$\n",
    "        - $N(s) \\leftarrow N(s) + 1$\n",
    "        - $V(s) \\leftarrow V(s) + (G - V(s))/n$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.11 - Why is the update of W $\\frac{1}{b(A_t|S_t}$ while you (likely) expected to be $\\frac{\\pi(A_t|S_t)}{b(A_t|S_t}$ from the weighted importance sampling?\n",
    "\n",
    "\n",
    "As the target-policy is the greedy policy and we have a condition to exit the inner loop of the algorithm in case $A_t != \\pi(S_t)$ (since in that case $\\pi(S_t) = 0$, thus rendering any continuation of the calculation meaningless), the algorithm just replaces $\\pi(S_t)$ to its numeric value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
